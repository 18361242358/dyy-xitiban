import collections
import d2lzh as d2l
from mxnet import gluon, init, nd
from mxnet.contrib import text
from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils
import os
import random
import tarfile
import mxnet as mx

def read_qth(folder='qthtrain'):  # 本函数已保存在d2lzh包中方便以后使用
    data = []
    for label in ['danger','normal']:#'else','electrical equipment','interference'
        folder_name = os.path.join('../data/', folder, label)
        for file in os.listdir(folder_name):
            with open(os.path.join(folder_name, file), 'rb') as f:
                review = f.read().decode('utf-8').replace('\n', '').lower()
                data.append([review, 1 if label == 'danger' else 0])
                '''if label == 'danger':
                    data.append([review, 1])
                #elif label == 'interference':
                         #data.append([review, 1])
                #elif label == 'normal':
                         #data.append([review, 3])
                #elif label == 'interference':
                         #data.append([review, 4])
                #elif label == 'roof support':
                         #data.append([review, 5])
                #elif label == 'sensor':
                         #data.append([review, 6])
                else:
                    data.append([review, 0])'''
    random.shuffle(data)
    return data

#train_data = read_das('train')#, read_imdb('test')
train_data, test_data = read_qth('qthtrain'), read_qth('qthtest')

def get_tokenized_qth(data):  # 本函数已保存在d2lzh包中方便以后使用
    def tokenizer(text):
        return [tok.lower() for tok in text.split(' ')]
    return [tokenizer(review) for review, _ in data]

def get_vocab_qth(data):  # 本函数已保存在d2lzh包中方便以后使用
    tokenized_data = get_tokenized_qth(data)
    counter = collections.Counter([tk for st in tokenized_data for tk in st])
    return text.vocab.Vocabulary(counter)#, min_freq=1)

vocab = get_vocab_qth(train_data)
'# words in vocab:', len(vocab)

def preprocess_qth(data, vocab):  # 本函数已保存在d2lzh包中方便以后使用
    max_l = 10  # 将每条评论通过截断或者补0，使得长度变成500

    def pad(x):
        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x)) 

    tokenized_data = get_tokenized_qth(data)
    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])
    labels = nd.array([score for _, score in data])
    return features, labels

batch_size = 5
train_set = gdata.ArrayDataset(*preprocess_qth(train_data, vocab))
test_set = gdata.ArrayDataset(*preprocess_qth(test_data, vocab))
train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)
test_iter = gdata.DataLoader(test_set, batch_size)

for X, y in train_iter:
    print('X', X.shape, 'y', y.shape)
    break
'#batches:', len(train_iter)

class BiRNN(nn.Block):
    def __init__(self, vocab, embed_size, num_hiddens, num_layers, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(len(vocab), embed_size)
        # bidirectional设为True即得到双向循环神经网络
        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,
                                bidirectional=True, input_size=embed_size)
        self.decoder = nn.Dense(2)

    def forward(self, inputs):
        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列作为第一维，所以将输入转置后
        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)
        embeddings = self.embedding(inputs.T)
        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。
        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)
        outputs = self.encoder(embeddings)
        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为
        # (批量大小, 4 * 隐藏单元个数)。
        encoding = nd.concat(outputs[0], outputs[-1])
        outs = self.decoder(encoding)
        return outs

embed_size, num_hiddens, num_layers, ctx = 10, 10, 2, mx.cpu()
net = BiRNN(vocab, embed_size, num_hiddens, num_layers)
net.initialize(init.Xavier(), ctx=ctx)

lr, num_epochs = 0.01, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gloss.SoftmaxCrossEntropyLoss()
d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)

def predict_qth(net, vocab, sentence):
    sentence = nd.array(vocab.to_indices(sentence), ctx=mx.cpu())
    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)
    '''if label.asscalar() == 1:
        return 'danger'
    #elif label.asscalar() == 1:
        #return 'interference'
    #elif label.asscalar() == 3:
       # return 'normal'#'roof support'#'cable'#'electrical equipment'#'drill'##'sensor'#'else'#'guns taken'
    #elif label.asscalar() == 4:
        #return 'drill'
    #elif label.asscalar() == 5:
        #return 'roof support'
    #elif label.asscalar() == 6:
        #return 'sensor'
    #else:
        return 'other'
        '''
    return 'danger' if label.asscalar() == 1 else 'other'

predict_qth(net, vocab, [43,62,26,51,51,56,71,54,48,48])

predict_qth(net, vocab, [62,26,51,51,56,71,54,48,48,55])

predict_qth(net, vocab, [26,51,51,56,71,54,48,48,55,49])

predict_qth(net, vocab, [51,51,56,71,54,48,48,55,49,53])

predict_qth(net, vocab, [51,16,49,9,61,56,48,53,49,50])

predict_qth(net, vocab, [16,49,9,61,56,48,53,49,50,52])

predict_qth(net, vocab, [49,9,61,56,48,53,49,50,52,50])

predict_qth(net, vocab, [9,61,56,48,53,49,50,52,50,48])

predict_qth(net, vocab, [51,56,71,54,48,48,55,49,53,51])

predict_qth(net, vocab, [5,38,24,51,62,10,7,6,43,7])

predict_qth(net, vocab, [38,24,51,62,10,7,6,43,7,6])

predict_qth(net, vocab, [8,7,9,9,6,10.5,85,82,11.5,15.5])

predict_qth(net, vocab, [5,43,7,9,70,18,47,12,11,51])

predict_qth(net, vocab, [17,22,23,10,25,19,380,141,23,22])

predict_qth(net, vocab, [17,16,20,19,19,20,20,19,19,19])

predict_qth(net, vocab, [16,20,19,19,20,20,19,19,19,19])

predict_qth(net, vocab, [20,19,19,20,20,19,19,19,19,20])

predict_qth(net, vocab, [19,19,19,19,20,20,19,20,20,18])

predict_qth(net, vocab, [19,19,19,20,20,19,20,20,18,13])
